{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph, optimize_for_inference_lib\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    vgg_model = keras.applications.VGG16(include_top=True, weights='imagenet')\n",
    "    vgg_model.layers.pop()\n",
    "    vgg_model.layers.pop()\n",
    "\n",
    "    inp = vgg_model.input\n",
    "    out = vgg_model.layers[-1].output\n",
    "\n",
    "    model = Model(inp, out)\n",
    "    return model\n",
    "\n",
    "def get_features(model, cropped_image):\n",
    "    x = image.img_to_array(cropped_image)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = keras.applications.vgg16.preprocess_input(x)\n",
    "    features = model.predict(x)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "import gzip\n",
    "import _pickle as cPickle\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, normalize\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "WORD2VECPATH    = \"data/class_vectors.npy\"\n",
    "DATAPATH        = \"data/zeroshot_data.pkl\"\n",
    "MODELPATH       = \"model/\"\n",
    "\n",
    "def load_keras_model(model_path):\n",
    "    with open(model_path +\"model.json\", 'r') as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(model_path+\"model.h5\")\n",
    "    return loaded_model\n",
    "\n",
    "def save_keras_model(model, model_path):\n",
    "    \"\"\"save Keras model and its weights\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    model_json = model.to_json()\n",
    "    with open(model_path + \"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_path + \"model.h5\")\n",
    "    print(\"-> zsl model is saved.\")\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"read data, create datasets\"\"\"\n",
    "    # READ DATA\n",
    "    with gzip.GzipFile(DATAPATH, 'rb') as infile:\n",
    "        data = cPickle.load(infile)\n",
    "\n",
    "    # ONE-HOT-ENCODE DATA\n",
    "    label_encoder   = LabelEncoder()\n",
    "    label_encoder.fit(train_classes)\n",
    "\n",
    "    training_data = [instance for instance in data if instance[0] in train_classes]\n",
    "    zero_shot_data = [instance for instance in data if instance[0] not in train_classes]\n",
    "    # SHUFFLE TRAINING DATA\n",
    "    np.random.shuffle(training_data)\n",
    "\n",
    "    ### SPLIT DATA FOR TRAINING\n",
    "    train_size  = 300\n",
    "    train_data  = list()\n",
    "    valid_data  = list()\n",
    "    for class_label in train_classes:\n",
    "        ct = 0\n",
    "        for instance in training_data:\n",
    "            if instance[0] == class_label:\n",
    "                if ct < train_size:\n",
    "                    train_data.append(instance)\n",
    "                    ct+=1\n",
    "                    continue\n",
    "                valid_data.append(instance)\n",
    "\n",
    "    # SHUFFLE TRAINING AND VALIDATION DATA\n",
    "    np.random.shuffle(train_data)\n",
    "    np.random.shuffle(valid_data)\n",
    "\n",
    "    train_data = [(instance[1], to_categorical(label_encoder.transform([instance[0]]), num_classes=15))for instance in train_data]\n",
    "    valid_data = [(instance[1], to_categorical(label_encoder.transform([instance[0]]), num_classes=15)) for instance in valid_data]\n",
    "\n",
    "    # FORM X_TRAIN AND Y_TRAIN\n",
    "    x_train, y_train    = zip(*train_data)\n",
    "    x_train, y_train    = np.squeeze(np.asarray(x_train)), np.squeeze(np.asarray(y_train))\n",
    "    # L2 NORMALIZE X_TRAIN\n",
    "    x_train = normalize(x_train, norm='l2')\n",
    "\n",
    "    # FORM X_VALID AND Y_VALID\n",
    "    x_valid, y_valid = zip(*valid_data)\n",
    "    x_valid, y_valid = np.squeeze(np.asarray(x_valid)), np.squeeze(np.asarray(y_valid))\n",
    "    # L2 NORMALIZE X_VALID\n",
    "    x_valid = normalize(x_valid, norm='l2')\n",
    "\n",
    "\n",
    "    # FORM X_ZSL AND Y_ZSL\n",
    "    y_zsl, x_zsl = zip(*zero_shot_data)\n",
    "    x_zsl, y_zsl = np.squeeze(np.asarray(x_zsl)), np.squeeze(np.asarray(y_zsl))\n",
    "    # L2 NORMALIZE X_ZSL\n",
    "    x_zsl = normalize(x_zsl, norm='l2')\n",
    "\n",
    "    print(\"-> data loading is completed.\")\n",
    "    return (x_train, x_valid, x_zsl), (y_train, y_valid, y_zsl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> data loading is completed.\n",
      "-> model building is completed.\n",
      "Train on 4500 samples, validate on 1493 samples\n",
      "Epoch 1/65\n",
      " - 1s - loss: 2.9265 - categorical_accuracy: 0.0904 - top_k_categorical_accuracy: 0.3900 - val_loss: 2.4661 - val_categorical_accuracy: 0.2465 - val_top_k_categorical_accuracy: 0.6859\n",
      "Epoch 2/65\n",
      " - 0s - loss: 2.5952 - categorical_accuracy: 0.1502 - top_k_categorical_accuracy: 0.5278 - val_loss: 2.2519 - val_categorical_accuracy: 0.3262 - val_top_k_categorical_accuracy: 0.7810\n",
      "Epoch 3/65\n",
      " - 0s - loss: 2.3967 - categorical_accuracy: 0.2098 - top_k_categorical_accuracy: 0.6200 - val_loss: 2.0624 - val_categorical_accuracy: 0.3724 - val_top_k_categorical_accuracy: 0.8359\n",
      "Epoch 4/65\n",
      " - 0s - loss: 2.2412 - categorical_accuracy: 0.2480 - top_k_categorical_accuracy: 0.6873 - val_loss: 1.8967 - val_categorical_accuracy: 0.3938 - val_top_k_categorical_accuracy: 0.8654\n",
      "Epoch 5/65\n",
      " - 0s - loss: 2.0793 - categorical_accuracy: 0.2860 - top_k_categorical_accuracy: 0.7564 - val_loss: 1.7693 - val_categorical_accuracy: 0.4159 - val_top_k_categorical_accuracy: 0.8788\n",
      "Epoch 6/65\n",
      " - 0s - loss: 1.9795 - categorical_accuracy: 0.3098 - top_k_categorical_accuracy: 0.7858 - val_loss: 1.6683 - val_categorical_accuracy: 0.4334 - val_top_k_categorical_accuracy: 0.8962\n",
      "Epoch 7/65\n",
      " - 0s - loss: 1.8718 - categorical_accuracy: 0.3389 - top_k_categorical_accuracy: 0.8096 - val_loss: 1.5968 - val_categorical_accuracy: 0.4407 - val_top_k_categorical_accuracy: 0.9069\n",
      "Epoch 8/65\n",
      " - 0s - loss: 1.7945 - categorical_accuracy: 0.3571 - top_k_categorical_accuracy: 0.8291 - val_loss: 1.5328 - val_categorical_accuracy: 0.4595 - val_top_k_categorical_accuracy: 0.9082\n",
      "Epoch 9/65\n",
      " - 0s - loss: 1.7240 - categorical_accuracy: 0.3807 - top_k_categorical_accuracy: 0.8536 - val_loss: 1.4881 - val_categorical_accuracy: 0.4709 - val_top_k_categorical_accuracy: 0.9109\n",
      "Epoch 10/65\n",
      " - 0s - loss: 1.6736 - categorical_accuracy: 0.3951 - top_k_categorical_accuracy: 0.8609 - val_loss: 1.4567 - val_categorical_accuracy: 0.4762 - val_top_k_categorical_accuracy: 0.9149\n",
      "Epoch 11/65\n",
      " - 0s - loss: 1.6228 - categorical_accuracy: 0.4178 - top_k_categorical_accuracy: 0.8764 - val_loss: 1.4214 - val_categorical_accuracy: 0.4970 - val_top_k_categorical_accuracy: 0.9196\n",
      "Epoch 12/65\n",
      " - 0s - loss: 1.5872 - categorical_accuracy: 0.4200 - top_k_categorical_accuracy: 0.8813 - val_loss: 1.4000 - val_categorical_accuracy: 0.4983 - val_top_k_categorical_accuracy: 0.9236\n",
      "Epoch 13/65\n",
      " - 0s - loss: 1.5608 - categorical_accuracy: 0.4409 - top_k_categorical_accuracy: 0.8849 - val_loss: 1.3747 - val_categorical_accuracy: 0.5111 - val_top_k_categorical_accuracy: 0.9230\n",
      "Epoch 14/65\n",
      " - 0s - loss: 1.5010 - categorical_accuracy: 0.4462 - top_k_categorical_accuracy: 0.8998 - val_loss: 1.3592 - val_categorical_accuracy: 0.5131 - val_top_k_categorical_accuracy: 0.9283\n",
      "Epoch 15/65\n",
      " - 0s - loss: 1.4977 - categorical_accuracy: 0.4444 - top_k_categorical_accuracy: 0.9049 - val_loss: 1.3309 - val_categorical_accuracy: 0.5198 - val_top_k_categorical_accuracy: 0.9303\n",
      "Epoch 16/65\n",
      " - 0s - loss: 1.4438 - categorical_accuracy: 0.4658 - top_k_categorical_accuracy: 0.9122 - val_loss: 1.3172 - val_categorical_accuracy: 0.5211 - val_top_k_categorical_accuracy: 0.9337\n",
      "Epoch 17/65\n",
      " - 0s - loss: 1.4368 - categorical_accuracy: 0.4662 - top_k_categorical_accuracy: 0.9111 - val_loss: 1.2993 - val_categorical_accuracy: 0.5265 - val_top_k_categorical_accuracy: 0.9317\n",
      "Epoch 18/65\n",
      " - 0s - loss: 1.4026 - categorical_accuracy: 0.4833 - top_k_categorical_accuracy: 0.9158 - val_loss: 1.2967 - val_categorical_accuracy: 0.5224 - val_top_k_categorical_accuracy: 0.9344\n",
      "Epoch 19/65\n",
      " - 0s - loss: 1.3739 - categorical_accuracy: 0.4938 - top_k_categorical_accuracy: 0.9227 - val_loss: 1.2797 - val_categorical_accuracy: 0.5365 - val_top_k_categorical_accuracy: 0.9330\n",
      "Epoch 20/65\n",
      " - 0s - loss: 1.3684 - categorical_accuracy: 0.4887 - top_k_categorical_accuracy: 0.9236 - val_loss: 1.2650 - val_categorical_accuracy: 0.5459 - val_top_k_categorical_accuracy: 0.9350\n",
      "Epoch 21/65\n",
      " - 0s - loss: 1.3406 - categorical_accuracy: 0.5016 - top_k_categorical_accuracy: 0.9247 - val_loss: 1.2555 - val_categorical_accuracy: 0.5439 - val_top_k_categorical_accuracy: 0.9370\n",
      "Epoch 22/65\n",
      " - 0s - loss: 1.3287 - categorical_accuracy: 0.5011 - top_k_categorical_accuracy: 0.9273 - val_loss: 1.2445 - val_categorical_accuracy: 0.5399 - val_top_k_categorical_accuracy: 0.9364\n",
      "Epoch 23/65\n",
      " - 0s - loss: 1.3199 - categorical_accuracy: 0.5082 - top_k_categorical_accuracy: 0.9276 - val_loss: 1.2377 - val_categorical_accuracy: 0.5445 - val_top_k_categorical_accuracy: 0.9417\n",
      "Epoch 24/65\n",
      " - 0s - loss: 1.2918 - categorical_accuracy: 0.5167 - top_k_categorical_accuracy: 0.9324 - val_loss: 1.2312 - val_categorical_accuracy: 0.5425 - val_top_k_categorical_accuracy: 0.9411\n",
      "Epoch 25/65\n",
      " - 0s - loss: 1.2721 - categorical_accuracy: 0.5262 - top_k_categorical_accuracy: 0.9349 - val_loss: 1.2267 - val_categorical_accuracy: 0.5425 - val_top_k_categorical_accuracy: 0.9411\n",
      "Epoch 26/65\n",
      " - 0s - loss: 1.2503 - categorical_accuracy: 0.5382 - top_k_categorical_accuracy: 0.9371 - val_loss: 1.2202 - val_categorical_accuracy: 0.5439 - val_top_k_categorical_accuracy: 0.9404\n",
      "Epoch 27/65\n",
      " - 0s - loss: 1.2280 - categorical_accuracy: 0.5424 - top_k_categorical_accuracy: 0.9442 - val_loss: 1.2130 - val_categorical_accuracy: 0.5486 - val_top_k_categorical_accuracy: 0.9397\n",
      "Epoch 28/65\n",
      " - 0s - loss: 1.2175 - categorical_accuracy: 0.5458 - top_k_categorical_accuracy: 0.9393 - val_loss: 1.2058 - val_categorical_accuracy: 0.5506 - val_top_k_categorical_accuracy: 0.9397\n",
      "Epoch 29/65\n",
      " - 0s - loss: 1.1959 - categorical_accuracy: 0.5480 - top_k_categorical_accuracy: 0.9449 - val_loss: 1.2023 - val_categorical_accuracy: 0.5492 - val_top_k_categorical_accuracy: 0.9411\n",
      "Epoch 30/65\n",
      " - 0s - loss: 1.1930 - categorical_accuracy: 0.5529 - top_k_categorical_accuracy: 0.9429 - val_loss: 1.1918 - val_categorical_accuracy: 0.5559 - val_top_k_categorical_accuracy: 0.9404\n",
      "Epoch 31/65\n",
      " - 0s - loss: 1.1591 - categorical_accuracy: 0.5709 - top_k_categorical_accuracy: 0.9496 - val_loss: 1.1884 - val_categorical_accuracy: 0.5546 - val_top_k_categorical_accuracy: 0.9424\n",
      "Epoch 32/65\n",
      " - 0s - loss: 1.1524 - categorical_accuracy: 0.5709 - top_k_categorical_accuracy: 0.9524 - val_loss: 1.1871 - val_categorical_accuracy: 0.5546 - val_top_k_categorical_accuracy: 0.9424\n",
      "Epoch 33/65\n",
      " - 0s - loss: 1.1565 - categorical_accuracy: 0.5684 - top_k_categorical_accuracy: 0.9460 - val_loss: 1.1820 - val_categorical_accuracy: 0.5559 - val_top_k_categorical_accuracy: 0.9424\n",
      "Epoch 34/65\n",
      " - 0s - loss: 1.1244 - categorical_accuracy: 0.5811 - top_k_categorical_accuracy: 0.9511 - val_loss: 1.1763 - val_categorical_accuracy: 0.5539 - val_top_k_categorical_accuracy: 0.9424\n",
      "Epoch 35/65\n",
      " - 0s - loss: 1.1097 - categorical_accuracy: 0.5876 - top_k_categorical_accuracy: 0.9531 - val_loss: 1.1701 - val_categorical_accuracy: 0.5613 - val_top_k_categorical_accuracy: 0.9424\n",
      "Epoch 36/65\n",
      " - 0s - loss: 1.0919 - categorical_accuracy: 0.5911 - top_k_categorical_accuracy: 0.9491 - val_loss: 1.1772 - val_categorical_accuracy: 0.5653 - val_top_k_categorical_accuracy: 0.9417\n",
      "Epoch 37/65\n",
      " - 0s - loss: 1.0801 - categorical_accuracy: 0.5998 - top_k_categorical_accuracy: 0.9558 - val_loss: 1.1718 - val_categorical_accuracy: 0.5586 - val_top_k_categorical_accuracy: 0.9411\n",
      "Epoch 38/65\n",
      " - 0s - loss: 1.0816 - categorical_accuracy: 0.5969 - top_k_categorical_accuracy: 0.9551 - val_loss: 1.1695 - val_categorical_accuracy: 0.5586 - val_top_k_categorical_accuracy: 0.9451\n",
      "Epoch 39/65\n",
      " - 0s - loss: 1.0379 - categorical_accuracy: 0.6122 - top_k_categorical_accuracy: 0.9616 - val_loss: 1.1595 - val_categorical_accuracy: 0.5633 - val_top_k_categorical_accuracy: 0.9437\n",
      "Epoch 40/65\n",
      " - 0s - loss: 1.0381 - categorical_accuracy: 0.6227 - top_k_categorical_accuracy: 0.9629 - val_loss: 1.1637 - val_categorical_accuracy: 0.5700 - val_top_k_categorical_accuracy: 0.9444\n",
      "Epoch 41/65\n",
      " - 0s - loss: 1.0263 - categorical_accuracy: 0.6158 - top_k_categorical_accuracy: 0.9616 - val_loss: 1.1607 - val_categorical_accuracy: 0.5687 - val_top_k_categorical_accuracy: 0.9437\n",
      "Epoch 42/65\n",
      " - 0s - loss: 1.0103 - categorical_accuracy: 0.6229 - top_k_categorical_accuracy: 0.9624 - val_loss: 1.1578 - val_categorical_accuracy: 0.5720 - val_top_k_categorical_accuracy: 0.9431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/65\n",
      " - 0s - loss: 1.0068 - categorical_accuracy: 0.6227 - top_k_categorical_accuracy: 0.9631 - val_loss: 1.1531 - val_categorical_accuracy: 0.5693 - val_top_k_categorical_accuracy: 0.9431\n",
      "Epoch 44/65\n",
      " - 0s - loss: 0.9767 - categorical_accuracy: 0.6342 - top_k_categorical_accuracy: 0.9669 - val_loss: 1.1492 - val_categorical_accuracy: 0.5740 - val_top_k_categorical_accuracy: 0.9457\n",
      "Epoch 45/65\n",
      " - 0s - loss: 0.9888 - categorical_accuracy: 0.6291 - top_k_categorical_accuracy: 0.9636 - val_loss: 1.1487 - val_categorical_accuracy: 0.5834 - val_top_k_categorical_accuracy: 0.9444\n",
      "Epoch 46/65\n",
      " - 0s - loss: 0.9603 - categorical_accuracy: 0.6404 - top_k_categorical_accuracy: 0.9664 - val_loss: 1.1461 - val_categorical_accuracy: 0.5807 - val_top_k_categorical_accuracy: 0.9451\n",
      "Epoch 47/65\n",
      " - 0s - loss: 0.9769 - categorical_accuracy: 0.6407 - top_k_categorical_accuracy: 0.9658 - val_loss: 1.1459 - val_categorical_accuracy: 0.5800 - val_top_k_categorical_accuracy: 0.9417\n",
      "Epoch 48/65\n",
      " - 0s - loss: 0.9573 - categorical_accuracy: 0.6431 - top_k_categorical_accuracy: 0.9667 - val_loss: 1.1506 - val_categorical_accuracy: 0.5780 - val_top_k_categorical_accuracy: 0.9417\n",
      "Epoch 49/65\n",
      " - 0s - loss: 0.9340 - categorical_accuracy: 0.6509 - top_k_categorical_accuracy: 0.9693 - val_loss: 1.1490 - val_categorical_accuracy: 0.5854 - val_top_k_categorical_accuracy: 0.9411\n",
      "Epoch 50/65\n",
      " - 0s - loss: 0.9156 - categorical_accuracy: 0.6689 - top_k_categorical_accuracy: 0.9702 - val_loss: 1.1479 - val_categorical_accuracy: 0.5854 - val_top_k_categorical_accuracy: 0.9404\n",
      "Epoch 51/65\n",
      " - 0s - loss: 0.9194 - categorical_accuracy: 0.6607 - top_k_categorical_accuracy: 0.9687 - val_loss: 1.1456 - val_categorical_accuracy: 0.5881 - val_top_k_categorical_accuracy: 0.9437\n",
      "Epoch 52/65\n",
      " - 0s - loss: 0.9183 - categorical_accuracy: 0.6547 - top_k_categorical_accuracy: 0.9684 - val_loss: 1.1470 - val_categorical_accuracy: 0.5928 - val_top_k_categorical_accuracy: 0.9390\n",
      "Epoch 53/65\n",
      " - 0s - loss: 0.9022 - categorical_accuracy: 0.6644 - top_k_categorical_accuracy: 0.9698 - val_loss: 1.1429 - val_categorical_accuracy: 0.5901 - val_top_k_categorical_accuracy: 0.9417\n",
      "Epoch 54/65\n",
      " - 0s - loss: 0.8839 - categorical_accuracy: 0.6736 - top_k_categorical_accuracy: 0.9733 - val_loss: 1.1447 - val_categorical_accuracy: 0.5881 - val_top_k_categorical_accuracy: 0.9431\n",
      "Epoch 55/65\n",
      " - 0s - loss: 0.8706 - categorical_accuracy: 0.6760 - top_k_categorical_accuracy: 0.9747 - val_loss: 1.1468 - val_categorical_accuracy: 0.5894 - val_top_k_categorical_accuracy: 0.9457\n",
      "Epoch 56/65\n",
      " - 0s - loss: 0.8520 - categorical_accuracy: 0.6818 - top_k_categorical_accuracy: 0.9778 - val_loss: 1.1487 - val_categorical_accuracy: 0.5894 - val_top_k_categorical_accuracy: 0.9471\n",
      "Epoch 57/65\n",
      " - 0s - loss: 0.8471 - categorical_accuracy: 0.6793 - top_k_categorical_accuracy: 0.9713 - val_loss: 1.1539 - val_categorical_accuracy: 0.5867 - val_top_k_categorical_accuracy: 0.9417\n",
      "Epoch 58/65\n",
      " - 0s - loss: 0.8179 - categorical_accuracy: 0.6944 - top_k_categorical_accuracy: 0.9776 - val_loss: 1.1449 - val_categorical_accuracy: 0.5901 - val_top_k_categorical_accuracy: 0.9437\n",
      "Epoch 59/65\n",
      " - 0s - loss: 0.8364 - categorical_accuracy: 0.6864 - top_k_categorical_accuracy: 0.9756 - val_loss: 1.1512 - val_categorical_accuracy: 0.5921 - val_top_k_categorical_accuracy: 0.9444\n",
      "Epoch 60/65\n",
      " - 0s - loss: 0.8200 - categorical_accuracy: 0.6942 - top_k_categorical_accuracy: 0.9780 - val_loss: 1.1562 - val_categorical_accuracy: 0.5901 - val_top_k_categorical_accuracy: 0.9451\n",
      "Epoch 61/65\n",
      " - 0s - loss: 0.8033 - categorical_accuracy: 0.6993 - top_k_categorical_accuracy: 0.9778 - val_loss: 1.1562 - val_categorical_accuracy: 0.5887 - val_top_k_categorical_accuracy: 0.9444\n",
      "Epoch 62/65\n",
      " - 0s - loss: 0.7944 - categorical_accuracy: 0.6971 - top_k_categorical_accuracy: 0.9780 - val_loss: 1.1518 - val_categorical_accuracy: 0.5914 - val_top_k_categorical_accuracy: 0.9451\n",
      "Epoch 63/65\n",
      " - 0s - loss: 0.7832 - categorical_accuracy: 0.7049 - top_k_categorical_accuracy: 0.9809 - val_loss: 1.1488 - val_categorical_accuracy: 0.5948 - val_top_k_categorical_accuracy: 0.9444\n",
      "Epoch 64/65\n",
      " - 0s - loss: 0.7680 - categorical_accuracy: 0.7238 - top_k_categorical_accuracy: 0.9800 - val_loss: 1.1499 - val_categorical_accuracy: 0.5948 - val_top_k_categorical_accuracy: 0.9444\n",
      "Epoch 65/65\n",
      " - 0s - loss: 0.7420 - categorical_accuracy: 0.7322 - top_k_categorical_accuracy: 0.9813 - val_loss: 1.1409 - val_categorical_accuracy: 0.6042 - val_top_k_categorical_accuracy: 0.9471\n",
      "model training is completed.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 300)               77100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 15)                4515      \n",
      "=================================================================\n",
      "Total params: 4,937,167\n",
      "Trainable params: 4,930,604\n",
      "Non-trainable params: 6,563\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1_input (InputLayer)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 300)               77100     \n",
      "=================================================================\n",
      "Total params: 4,932,652\n",
      "Trainable params: 4,930,604\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "None\n",
      "-> zsl model is saved.\n",
      "\n",
      "ZERO SHOT LEARNING SCORE\n",
      "-> Top-5 Accuracy: 0.79\n",
      "-> Top-3 Accuracy: 0.39\n",
      "-> Top-1 Accuracy: 0.15\n"
     ]
    }
   ],
   "source": [
    "def custom_kernel_init(shape):\n",
    "    class_vectors       = np.load(WORD2VECPATH)\n",
    "    training_vectors    = sorted([(label, vec) for (label, vec) in class_vectors if label in train_classes], key=lambda x: x[0])\n",
    "    classnames, vectors = zip(*training_vectors)\n",
    "    vectors             = np.asarray(vectors, dtype=np.float)\n",
    "    vectors             = vectors.T\n",
    "    return vectors\n",
    "\n",
    "def  build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_shape=(4096,), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.8))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(NUM_ATTR, activation='relu'))\n",
    "    model.add(Dense(NUM_CLASS, activation='softmax', trainable=False, kernel_initializer=custom_kernel_init))\n",
    "\n",
    "    print(\"-> model building is completed.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, train_data, valid_data):\n",
    "    x_train, y_train = train_data\n",
    "    x_valid, y_valid = valid_data\n",
    "    adam = Adam(lr=5e-5)\n",
    "    model.compile(loss      = 'categorical_crossentropy',\n",
    "                  optimizer = adam,\n",
    "                  metrics   = ['categorical_accuracy', 'top_k_categorical_accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        validation_data = (x_valid, y_valid),\n",
    "                        verbose         = 2,\n",
    "                        epochs          = EPOCH,\n",
    "                        batch_size      = BATCH_SIZE,\n",
    "                        shuffle         = True)\n",
    "\n",
    "    print(\"model training is completed.\")\n",
    "    return history\n",
    "\n",
    "def main():\n",
    "\n",
    "    global train_classes\n",
    "    with open('train_classes.txt', 'r') as infile:\n",
    "        train_classes = [str.strip(line) for line in infile]\n",
    "\n",
    "    global zsl_classes\n",
    "    with open('zsl_classes.txt', 'r') as infile:\n",
    "        zsl_classes = [str.strip(line) for line in infile]\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------------------- #\n",
    "    # ---------------------------------------------------------------------------------------------------------------- #\n",
    "    # SET HYPERPARAMETERS\n",
    "\n",
    "    global NUM_CLASS, NUM_ATTR, EPOCH, BATCH_SIZE\n",
    "    NUM_CLASS = 15\n",
    "    NUM_ATTR = 300\n",
    "    BATCH_SIZE = 128\n",
    "    EPOCH = 65\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------------------- #\n",
    "    # ---------------------------------------------------------------------------------------------------------------- #\n",
    "    # TRAINING PHASE\n",
    "\n",
    "    (x_train, x_valid, x_zsl), (y_train, y_valid, y_zsl) = load_data()\n",
    "    model = build_model()\n",
    "    train_model(model, (x_train, y_train), (x_valid, y_valid))\n",
    "    print(model.summary())\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------------------- #\n",
    "    # ---------------------------------------------------------------------------------------------------------------- #\n",
    "    # CREATE AND SAVE ZSL MODEL\n",
    "\n",
    "    inp         = model.input\n",
    "    out         = model.layers[-2].output\n",
    "    zsl_model   = Model(inp, out)\n",
    "    print(zsl_model.summary())\n",
    "    save_keras_model(zsl_model, model_path=MODELPATH)\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------------------- #\n",
    "    # ---------------------------------------------------------------------------------------------------------------- #\n",
    "    # EVALUATION OF ZERO-SHOT LEARNING PERFORMANCE\n",
    "    #(x_train, x_valid, x_zsl), (y_train, y_valid, y_zsl) = load_data()\n",
    "    #zsl_model = load_keras_model(model_path=MODELPATH)\n",
    "\n",
    "    class_vectors       = sorted(np.load(WORD2VECPATH), key=lambda x: x[0])\n",
    "    classnames, vectors = zip(*class_vectors)\n",
    "    classnames          = list(classnames)\n",
    "    vectors             = np.asarray(vectors, dtype=np.float)\n",
    "\n",
    "    tree        = KDTree(vectors)\n",
    "    pred_zsl    = zsl_model.predict(x_zsl)\n",
    "\n",
    "    top5, top3, top1 = 0, 0, 0\n",
    "    for i, pred in enumerate(pred_zsl):\n",
    "        pred            = np.expand_dims(pred, axis=0)\n",
    "        dist_5, index_5 = tree.query(pred, k=5)\n",
    "        pred_labels     = [classnames[index] for index in index_5[0]]\n",
    "        true_label      = y_zsl[i]\n",
    "        if true_label in pred_labels:\n",
    "            top5 += 1\n",
    "        if true_label in pred_labels[:3]:\n",
    "            top3 += 1\n",
    "        if true_label in pred_labels[0]:\n",
    "            top1 += 1\n",
    "\n",
    "    print()\n",
    "    print(\"ZERO SHOT LEARNING SCORE\")\n",
    "    print(\"-> Top-5 Accuracy: %.2f\" % (top5 / float(len(x_zsl))))\n",
    "    print(\"-> Top-3 Accuracy: %.2f\" % (top3 / float(len(x_zsl))))\n",
    "    print(\"-> Top-1 Accuracy: %.2f\" % (top1 / float(len(x_zsl))))\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: python3 detect_object.py input-image-path\n",
      "(1, 4096)\n",
      "[[0.00744639 0.05892316 0.         ... 0.         0.         0.        ]]\n",
      "\n",
      "--- Top-5 Prediction ---\n",
      "1- computer\n",
      "2- sandwich\n",
      "3- food\n",
      "4- woman\n",
      "5- child\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# detect_object.py\n",
    "#\n",
    "# Created by Samet Cetin.\n",
    "# Contact: cetin.samet@outlook.com\n",
    "#\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from feature_extractor import get_model, get_features\n",
    "from train import load_keras_model\n",
    "\n",
    "\n",
    "WORD2VECPATH    = \"data/class_vectors.npy\"\n",
    "MODELPATH       = \"model/\"\n",
    "\n",
    "def main(argv):\n",
    "\n",
    "    if len(argv) != 1:\n",
    "        print(\"Usage: python3 detect_object.py input-image-path\")\n",
    "        exit()\n",
    "\n",
    "    # READ IMAGE\n",
    "    IMAGEPATH = 'test6.jpg'\n",
    "    img         = Image.open(IMAGEPATH).resize((224, 224))\n",
    "\n",
    "    # LOAD PRETRAINED VGG16 MODEL FOR FEATURE EXTRACTION\n",
    "    vgg_model   = get_model()\n",
    "    # EXTRACT IMAGE FEATURE\n",
    "    img_feature = get_features(vgg_model, img)\n",
    "    \n",
    "    print(img_feature.shape)\n",
    "    # L2 NORMALIZE FEATURE\n",
    "    img_feature = normalize(img_feature, norm='l2')\n",
    "    \n",
    "    print(img_feature)\n",
    "    \n",
    "    # LOAD ZERO-SHOT MODEL\n",
    "    model       = load_keras_model(model_path=MODELPATH)\n",
    "    # MAKE PREDICTION\n",
    "    pred        = model.predict(img_feature)\n",
    "\n",
    "    # LOAD CLASS WORD2VECS\n",
    "    class_vectors       = sorted(np.load(WORD2VECPATH), key=lambda x: x[0])\n",
    "    classnames, vectors = zip(*class_vectors)\n",
    "    classnames          = list(classnames)\n",
    "    vectors             = np.asarray(vectors, dtype=np.float)\n",
    "\n",
    "    # PLACE WORD2VECS IN KDTREE\n",
    "    tree                = KDTree(vectors)\n",
    "    # FIND CLOSEST WORD2VEC and GET PREDICTION RESULT\n",
    "    dist, index         = tree.query(pred, k=5)\n",
    "    pred_labels         = [classnames[idx] for idx in index[0]]\n",
    "\n",
    "    # PRINT RESULT\n",
    "    print()\n",
    "    print(\"--- Top-5 Prediction ---\")\n",
    "    for i, classname in enumerate(pred_labels):\n",
    "        print(\"%d- %s\" %(i+1, classname))\n",
    "    print()\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
